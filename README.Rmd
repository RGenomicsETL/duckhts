---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```
# DuckHTS

A [DuckDB](https://duckdb.org/) extension (see the [DuckDB Extension API](https://duckdb.org/docs/extensions/overview)) for reading high-throughput sequencing (HTS)
file formats using [htslib](https://github.com/samtools/htslib).

Query VCF, BCF, BAM, CRAM, FASTA, FASTQ, GTF, GFF, and tabix-indexed files directly using SQL.

Note: MSVC builds (windows_amd64/windows_arm64) are not supported. Use MinGW/RTools for Windows.

## Functions

| Function | Description | Schema |
|---|---|---|
| `read_bcf(path, [region, tidy_format])` | Read VCF/BCF files | CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO_*, FORMAT_* |
| `read_bam(path, [region, reference, standard_tags, auxiliary_tags])` | Read SAM/BAM/CRAM files | QNAME, FLAG, RNAME, POS, MAPQ, CIGAR, RNEXT, PNEXT, TLEN, SEQ, QUAL, READ_GROUP_ID, SAMPLE_ID (+ SAMtags / AUX) |
| `read_fasta(path, [region, index_path])` | Read FASTA files (full scan or indexed regions) | NAME, DESCRIPTION, SEQUENCE |
| `read_fastq(path, [mate_path, interleaved])` | Read FASTQ files | NAME, DESCRIPTION, SEQUENCE, QUALITY (+ MATE, PAIR_ID when paired/interleaved) |
| `fasta_index(path, [index_path])` | Build FASTA index (.fai) | success, index_path |
| `read_gff(path, [region, attributes_map])` | Read GFF3 files | seqname, source, feature, start, end, score, strand, frame, attributes (+ attributes_map MAP when enabled) |
| `read_gtf(path, [region, attributes_map])` | Read GTF files | seqname, source, feature, start, end, score, strand, frame, attributes (+ attributes_map MAP when enabled) |
| `read_tabix(path, [region, header, header_names, auto_detect, column_types])` | Read any tabix-indexed file | column0, column1, … (auto-detected) |
| `read_hts_header(path, [format, mode])` | Read HTS header metadata (`mode`: parsed/raw/both) | parsed: file_format, record_type, id, number, value_type, key_values; raw: idx, raw |
| `read_hts_index(path, [format, index_path])` | Read index metadata | file_format, seqname, tid, length, mapped, unmapped, index_type, meta |
| `read_hts_index_spans(path, [format, index_path])` | Read span-oriented index metadata view | file_format, seqname, tid, bin, chunk_beg_vo, chunk_end_vo, chunk_bytes, index_type, meta |
| `read_hts_index_raw(path, [format, index_path])` | Read raw index metadata blob view | index_type, index_path, raw |

`read_fastq` with `mate_path` requires exact QNAME pairing. `read_bam` supports typed `standard_tags` and `auxiliary_tags` maps. `read_tabix` supports header-aware parsing (`header`, `header_names`) and optional type inference (`auto_detect`, `column_types`). Region lists in comma-separated form are supported by `read_bam`, `read_bcf`, `read_fasta`, `read_gff`, `read_gtf`, and `read_tabix`. `read_bam` multi-region queries are deduplicated by htslib, while `read_bcf`/`read_fasta`/`read_gff`/`read_gtf`/`read_tabix` chain regions and can return duplicates for overlaps.

## Examples

The examples below run directly against bundled local test files and show the main reader APIs, including FASTA indexing and region queries.

```{r eval=TRUE}
library(DBI)
library(duckdb)

drv <- duckdb::duckdb(config = list(allow_unsigned_extensions = "true"))
con <- dbConnect(drv, dbdir = ":memory:")
ext_path <- normalizePath("build/release/duckhts.duckdb_extension", mustWork = FALSE)
dbExecute(con, sprintf("LOAD '%s'", ext_path))

dbGetQuery(con, "
  SELECT CHROM, POS, REF, ALT
  FROM read_bcf('test/data/formatcols.vcf.gz', tidy_format := true)
  LIMIT 3
")

dbGetQuery(con, "
  SELECT count(*) AS n
  FROM read_bam('test/data/range.bam', region := 'CHROMOSOME_I:1-1000')
")

dbGetQuery(con, "
  SELECT * FROM fasta_index('test/data/ce.fa')
")

dbGetQuery(con, "
  SELECT NAME, length(SEQUENCE) AS seq_length
  FROM read_fasta('test/data/ce.fa', region := 'CHROMOSOME_I:1-25')
")

dbGetQuery(con, "
  SELECT NAME, MATE, PAIR_ID
  FROM read_fastq('test/data/interleaved.fq', interleaved := true)
  LIMIT 3
")

dbDisconnect(con, shutdown = TRUE)
```

## Remote URLs and HTS_PATH

Remote URLs (S3/GCS/HTTP/S) are supported when htslib is built with plugins enabled. htslib loads these plugins from the directory specified by the `HTS_PATH` environment variable. Set `HTS_PATH` **before** loading the extension so the plugins can be discovered.

Example (using the packaged htslib plugins directory):

```{bash, eval=TRUE}
# Not run by default because it requires network access and a built extension.
extension_path=$(Rscript  --quiet -e 'cat(Rduckhts:::duckhts_extension_dir(),sep="")' )/build/duckhts.duckdb_extension
export HTS_PATH=$(Rscript --quiet -e 'cat(Rduckhts:::duckhts_htslib_plugins_dir(),sep="")')
duckdb -unsigned <<SQL
LOAD '${extension_path}';
SELECT CHROM, COUNT(*) AS n
FROM read_bcf('s3://1000genomes-dragen-v3.7.6/data/cohorts/gvcf-genotyper-dragen-3.7.6/hg19/3202-samples-cohort/3202_samples_cohort_gg_chr22.vcf.gz',
              region := 'chr22:16050000-16050500')
GROUP BY CHROM;
SQL
```

On Windows (MinGW/RTools), plugins are typically disabled so remote URLs generally do not work. Set `HTS_PATH` before loading the extension; if you change it later, restart the session and reload.

If you don’t have htslib plugins installed locally, download the prebuilt binaries from the r-universe-binaries GitHub release and point `HTS_PATH` at the extracted htslib/libexec/htslib directory inside the package bundle.
https://github.com/RGenomicsETL/duckhts/releases/tag/r-universe-binaries

### S3 credentials and configuration

The htslib S3 plugin supports credentials embedded in the URL or provided via environment variables or standard credentials files. For AWS-style credentials, the most common variables are:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (optional, for temporary credentials)
- `AWS_DEFAULT_REGION`
- `AWS_PROFILE` / `AWS_DEFAULT_PROFILE`
- `AWS_SHARED_CREDENTIALS_FILE` (override credentials file location)

You can also configure htslib-specific settings like `HTS_S3_ADDRESS_STYLE`, `HTS_S3_HOST`, and `HTS_S3_S3CFG` for non-default S3 endpoints or path-style access.

See the htslib S3 plugin documentation for full details, URL syntax, and short‑lived credentials support:
https://www.htslib.org/doc/htslib-s3-plugin.html

## Building

### Environment setup

Run the one-time configure step to create the Python venv and detect platform settings:

```bash
make configure
```

Note: MSVC builds (windows_amd64/windows_arm64) are not supported. Use MinGW/RTools for Windows.

### Prerequisites

- C compiler (GCC or Clang)
- CMake ≥ 3.5
- Make
- Python 3 + venv
- Git
- [htslib](https://github.com/samtools/htslib) build dependencies: zlib, libbz2, liblzma, libdeflate, libcurl, libcrypto (OpenSSL)

On Debian/Ubuntu:

```bash
sudo apt install build-essential cmake python3 python3-venv git \
    zlib1g-dev libbz2-dev liblzma-dev libdeflate-dev libcurl4-openssl-dev libssl-dev
```

On macOS:

```bash
brew install cmake htslib xz libdeflate
```

### Vendor [htslib](https://github.com/samtools/htslib)

```bash
./scripts/vendor_htslib.sh
```

This downloads and verifies [htslib](https://github.com/samtools/htslib) 1.23 into `third_party/htslib/`.

### Build

```bash
make configure    # one-time setup (Python venv, platform detection)
make release      # build optimised extension
```

The build runs [htslib](https://github.com/samtools/htslib)'s Makefile (`make lib-static`) in-tree.

The extension binary is written to `build/release/duckhts.duckdb_extension`.

### Debug build

```bash
make debug
```

## Loading

```sql
-- Unsigned extensions must be loaded with -unsigned flag:
-- duckdb -unsigned

LOAD '/path/to/duckhts.duckdb_extension';
```

## Testing

SQL tests live in `test/sql/` using [DuckDB](https://duckdb.org/)'s SQLLogicTest format.

Before running tests for the first time, prepare the indexed test data:

```bash
./test/scripts/prepare_test_data.sh   # requires samtools, bcftools, bgzip, tabix
```

This copies files from the vendored [htslib](https://github.com/samtools/htslib) test suite into `test/data/` and
builds the required indexes (BAI, CSI, TBI, FAI) so region queries work.

Then run:

```bash
make test_release
```

## R demo

The R package lives under `r/Rduckhts` and provides helpers to load the extension
and create [DuckDB](https://duckdb.org/) tables from HTS files. See its README for R-specific usage:
[r/Rduckhts/README.Rmd](r/Rduckhts/README.Rmd).

```{r eval=TRUE}
library(DBI)
library(duckdb)

drv <- duckdb::duckdb(config = list(allow_unsigned_extensions = "true"))
con <- dbConnect(drv, dbdir = ":memory:")
ext_path <- normalizePath("build/release/duckhts.duckdb_extension", mustWork = FALSE)
dbExecute(con, sprintf("LOAD '%s'", ext_path))

dbGetQuery(con, "
  SELECT *
  FROM read_bcf('test/data/formatcols.vcf.gz', tidy_format := true)
  LIMIT 5
")

parquet_path <- tempfile(fileext = ".parquet")
dbExecute(con, sprintf(
  "COPY (SELECT * FROM read_bcf('test/data/formatcols.vcf.gz', tidy_format := true)) TO '%s' (FORMAT PARQUET)",
  parquet_path
))
file.exists(parquet_path)

dbGetQuery(con, "
  SELECT NAME, SEQUENCE, QUALITY, MATE, PAIR_ID
  FROM read_fastq('test/data/r1.fq', mate_path := 'test/data/r2.fq')
  LIMIT 5
")

dbGetQuery(con, "
  SELECT QNAME, RNAME, POS, READ_GROUP_ID, SAMPLE_ID
  FROM read_bam('test/data/rg.sam.gz')
  LIMIT 5
")

dbGetQuery(con, "
  SELECT idx, raw
  FROM read_hts_header('test/data/formatcols.vcf.gz', mode := 'raw')
  LIMIT 3
")

dbGetQuery(con, "
  SELECT seqname, tid, index_type, chunk_beg_vo, chunk_end_vo
  FROM read_hts_index_spans('test/data/formatcols.vcf.gz')
  LIMIT 3
")

dbGetQuery(con, "
  SELECT index_type, octet_length(raw) AS raw_bytes
  FROM read_hts_index_raw('test/data/formatcols.vcf.gz')
")
```

### SAMtags + auxiliary tags

Standard SAMtags can be surfaced as typed columns and non-standard tags
captured in a map for ad hoc access:

```{r eval=TRUE}
dbGetQuery(con, "
  SELECT RG, NM, map_extract(AUXILIARY_TAGS, 'XZ') AS XZ
  FROM read_bam('test/data/aux_tags.sam.gz', standard_tags := true, auxiliary_tags := true)
  LIMIT 1
")

dbGetQuery(con, "
  SELECT seqname, feature, start, \"end\", attributes_map
  FROM read_gff('test/data/gff_file.gff.gz', attributes_map := true)
  WHERE feature = 'gene'
  LIMIT 5
")

dbGetQuery(con, "
  SELECT column0, column1
  FROM read_tabix('test/data/meta_tabix.tsv.gz')
  LIMIT 2
")

dbGetQuery(con, "
  SELECT chrom, pos
  FROM read_tabix('test/data/header_tabix.tsv.gz', header := true)
  LIMIT 2
")

dbGetQuery(con, "
  SELECT typeof(column1) AS column1_type
  FROM read_tabix('test/data/meta_tabix.tsv.gz', auto_detect := true)
  LIMIT 1
")

dbGetQuery(con, "
  SELECT pos + 1 AS pos_plus_one
  FROM read_tabix('test/data/header_tabix.tsv.gz', header := true,
                  column_types := ['VARCHAR','BIGINT','VARCHAR'])
  LIMIT 1
")

dbDisconnect(con, shutdown = TRUE)
```

## Project Structure

```
src/
  duckhts.c          # Extension entry point
  bcf_reader.c       # VCF/BCF reader (read_bcf)
  bam_reader.c       # SAM/BAM/CRAM reader (read_bam)
  seq_reader.c       # FASTA/FASTQ reader (read_fasta, read_fastq)
  tabix_reader.c     # Tabix/GTF/GFF reader (read_tabix, read_gtf, read_gff)
  vep_parser.c       # VEP/CSQ annotation parser
  include/
    vcf_types.h
    vep_parser.h
third_party/
  htslib/            # Vendored htslib 1.23 (built automatically)
test/
  sql/               # SQL logic tests
duckdb_capi/
  duckdb.h           # DuckDB C API headers
  duckdb_extension.h
r/
  Rduckhts/          # R package harness
```

## References

- DuckDB: https://duckdb.org/
- DuckDB Extension API: https://duckdb.org/docs/extensions/overview
- DuckDB extension template (C): https://github.com/duckdb/extension-template-c
- htslib: https://github.com/samtools/htslib
- RBCFTools: https://github.com/RGenomicsETL/RBCFTools

## License

MIT
